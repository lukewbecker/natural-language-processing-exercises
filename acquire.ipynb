{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://codeup.com/codeups-data-science-career-accelerator-is-here/'\n",
    "headers = {'User-Agent': 'Codeup Data Science'} # Some websites don't accept the pyhon-requests default user-agent\n",
    "response = get(url, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.text[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = soup.find('div', class_='jupiterx-post-content')\n",
    "article.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('article.txt', 'w') as f:\n",
    "    f.write(article.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_text(url):\n",
    "    # if we already have the data, read it locally\n",
    "    if os.path.exists('article.txt'):\n",
    "        with open('article.txt') as f:\n",
    "            return f.read()\n",
    "\n",
    "    # otherwise go fetch the data\n",
    "    url = url\n",
    "    headers = {'User-Agent': 'Codeup Data Science'}\n",
    "    response = get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    article = soup.find('div', class_='jupiterx-post-content')\n",
    "\n",
    "    # save it for next time\n",
    "    with open('article.txt', 'w') as f:\n",
    "        f.write(article.text)\n",
    "\n",
    "    return article.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercieses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codeup Blog Articles\n",
    "\n",
    "Scrape the article text from the following pages:\n",
    "- https://codeup.com/codeups-data-science-career-accelerator-is-here/\n",
    "- https://codeup.com/data-science-myths/\n",
    "- https://codeup.com/data-science-vs-data-analytics-whats-the-difference/\n",
    "- https://codeup.com/10-tips-to-crush-it-at-the-sa-tech-job-fair/\n",
    "- https://codeup.com/competitor-bootcamps-are-closing-is-the-model-in-danger/\n",
    "\n",
    "Encapsulate your work in a function named get_blog_articles that will return a list of dictionaries, with each dictionary representing one article. The shape of each dictionary should look like this:\n",
    "\n",
    "`{\n",
    "    'title': 'the title of the article',\n",
    "    'content': 'the full text content of the article'\n",
    "}`\n",
    "\n",
    "Plus any additional properties you think might be helpful.\n",
    "\n",
    "##### Bonus:\n",
    "\n",
    "- Scrape the text of all the articles linked on codeup's blog page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up a list of urls:\n",
    "\n",
    "urls = ['https://codeup.com/codeups-data-science-career-accelerator-is-here/', 'https://codeup.com/data-science-myths/', 'https://codeup.com/data-science-vs-data-analytics-whats-the-difference/', 'https://codeup.com/10-tips-to-crush-it-at-the-sa-tech-job-fair/', 'https://codeup.com/competitor-bootcamps-are-closing-is-the-model-in-danger/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_soup(url):\n",
    "    '''\n",
    "    This helper function takes in a url and requests and parses HTML\n",
    "    returning a soup object.\n",
    "    '''\n",
    "    headers = {'User-Agent': 'Codeup Data Science'} \n",
    "    response = get(url, headers=headers)    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_urls():\n",
    "    \n",
    "    url = 'https://codeup.com/resources/#blog'\n",
    "    \n",
    "    soup = make_soup(url)\n",
    "    \n",
    "    urls_list = soup.find_all('a', class_='jet-listing-dynamic-link__link')\n",
    "    \n",
    "    urls = {link.get('href') for link in urls_list}\n",
    "\n",
    "    urls = list(urls)\n",
    "        \n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faith's way:\n",
    "\n",
    "def get_blog_articles(urls, cached=False):\n",
    "    '''\n",
    "    This function takes in a list of Codeup Blog urls and a parameter\n",
    "    with default cached == False which scrapes the title and text for each url, \n",
    "    creates a list of dictionaries with the title and text for each blog, \n",
    "    converts list to df, and returns df.\n",
    "    If cached == True, the function returns a df from a json file.\n",
    "    '''\n",
    "    if cached == True:\n",
    "        df = pd.read_json('big_blogs.json')\n",
    "        \n",
    "    # cached == False completes a fresh scrape for df     \n",
    "    else:\n",
    "\n",
    "        # Create an empty list to hold dictionaries\n",
    "        articles = []\n",
    "\n",
    "        # Loop through each url in our list of urls\n",
    "        for url in urls:\n",
    "\n",
    "            # Make request and soup object using helper\n",
    "            soup = make_soup(url)\n",
    "\n",
    "            # Save the title of each blog in variable title\n",
    "            title = soup.find('h1').text\n",
    "\n",
    "            # Save the text in each blog to variable text\n",
    "            content = soup.find('div', class_=\"jupiterx-post-content\").text\n",
    "\n",
    "            # Create a dictionary holding the title and content for each blog\n",
    "            article = {'title': title, 'content': content}\n",
    "\n",
    "            # Add each dictionary to the articles list of dictionaries\n",
    "            articles.append(article)\n",
    "            \n",
    "        # convert our list of dictionaries to a df\n",
    "        df = pd.DataFrame(articles)\n",
    "\n",
    "        # Write df to a json file for faster access\n",
    "        df.to_json('big_blogs.json')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was my original get articles, but it's not as robust as the primary way that Faith showed me.\n",
    "\n",
    "urls = ['https://codeup.com/codeups-data-science-career-accelerator-is-here/', 'https://codeup.com/data-science-myths/', 'https://codeup.com/data-science-vs-data-analytics-whats-the-difference/', 'https://codeup.com/10-tips-to-crush-it-at-the-sa-tech-job-fair/','https://codeup.com/competitor-bootcamps-are-closing-is-the-model-in-danger/']\n",
    "def get_blog_articles_luke(url_list, cached = False):\n",
    "    final = [] \n",
    "    for x in url_list:\n",
    "        url = x\n",
    "        headers = {'User-Agent': 'Codeup Data Science'} # Some websites don't accept the pyhon-requests default user-agent\n",
    "        response = get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        article_title = soup.title.string\n",
    "        article = soup.find('div', class_='jupiterx-post-content')\n",
    "        article_text = article.text\n",
    "        item = {\n",
    "            'title': article_title,\n",
    "            'content': article_text\n",
    "        }\n",
    "        final.append(item)\n",
    "        \n",
    "        \n",
    "    df = pd.DataFrame(final)\n",
    "    \n",
    "    df.to_json('big_blogs.json')\n",
    "        \n",
    "        \n",
    "            # save it for next time\n",
    "    with open('article.txt', 'w') as f:\n",
    "        f.write(article.text)\n",
    "    return final\n",
    "\n",
    "# Big thanks to Matt for his help getting me pointed in the right direction on this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Big thanks to Matt for his help getting me pointed in the right direction on this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_list = get_blog_articles(urls)\n",
    "article_list.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1st webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing it manually, no dictionary... these are the steps to create the function and understand it.\n",
    "\n",
    "url = 'https://codeup.com/codeups-data-science-career-accelerator-is-here/'\n",
    "headers = {'User-Agent': 'Codeup Data Science'}\n",
    "response = get(url, headers = headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup.title.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the same thing, review this SO article: https://stackoverflow.com/questions/35496332/differences-between-text-and-get-text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News Articles\n",
    "\n",
    "We will now be scraping text data from inshorts, a website that provides a brief overview of many different topics.\n",
    "\n",
    "Write a function that scrapes the news articles for the following topics:\n",
    "\n",
    "- Business\n",
    "- Sports\n",
    "- Technology\n",
    "- Entertainment\n",
    "\n",
    "The end product of this should be a function named get_news_articles that returns a list of dictionaries, where each dictionary has this shape:\n",
    "\n",
    "`{\n",
    "    'title': 'The article title',\n",
    "    'content': 'The article content',\n",
    "    'category': 'business' # for example\n",
    "}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start simple; function that handles a single article and returns the dictionary I need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://inshorts.com/en/read'\n",
    "headers = {'User-Agent': 'Codeup Data Science'}\n",
    "response = get(url, headers=headers)\n",
    "# soup = BeautifulSoup(response.text)\n",
    "# article = soup.find('div', class_='jupiterx-post-content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = soup.find('div', class_='articleBody')\n",
    "article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://inshorts.com/en/read'\n",
    "headers = {'User-Agent': 'Codeup Data Science'}\n",
    "response = get(url, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_content = soup.find('div', itemprop='articleBody').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('span', itemprop='headline').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_test = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_test.title.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.prettify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a function that allows \n",
    "\n",
    "source_urls = ['https://inshorts.com/en/read/technology',\n",
    "             'https://inshorts.com/en/read/sports',\n",
    "             'https://inshorts.com/en/read/business',\n",
    "             'https://inshorts.com/en/read/entertainment']\n",
    "\n",
    "def build_news_dataset(source_urls):\n",
    "    news_data = []\n",
    "    for url in source_urls:\n",
    "        news_category = url.split('/')[-1]\n",
    "        data = get(url)\n",
    "        soup = BeautifulSoup(data.content, 'html.parser')\n",
    "        \n",
    "        news_articles = [{'title': headline.find('span', \n",
    "                                                         attrs={\"itemprop\": \"headline\"}).string,\n",
    "                          'content': article.find('div', \n",
    "                                                       attrs={\"itemprop\": \"articleBody\"}).string,\n",
    "                          'category': news_category}\n",
    "                         \n",
    "                            for headline, article in \n",
    "                             zip(soup.find_all('div', \n",
    "                                               class_=[\"news-card-title news-right-box\"]),\n",
    "                                 soup.find_all('div', \n",
    "                                               class_=[\"news-card-content news-right-box\"]))\n",
    "                        ]\n",
    "        news_data.extend(news_articles)\n",
    "        \n",
    "    df =  pd.DataFrame(news_data)\n",
    "    df = df[['title', 'content', 'category']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = build_news_dataset(source_urls)\n",
    "news_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete. I'm going to look at the shape of my text:\n",
    "\n",
    "news_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
